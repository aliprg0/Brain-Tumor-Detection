{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliprg0/Brain-Tumor-Detection/blob/main/CMm_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_5bxbCoe9do9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6989b236-4a32-4e4c-f39a-ca0862ce37b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 7.5 MB/s \n",
            "\u001b[?25hCollecting requests>=2.26\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.8.0 requests-2.27.1 yfinance-0.1.70\n",
            "Collecting yahooquery\n",
            "  Downloading yahooquery-2.2.15-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting requests-futures>=1.0.0\n",
            "  Downloading requests_futures-1.0.0-py2.py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (4.64.0)\n",
            "Requirement already satisfied: lxml>=4.6.2 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (4.8.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yahooquery) (1.15.0)\n",
            "Requirement already satisfied: requests>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from requests-futures>=1.0.0->yahooquery) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2.0.12)\n",
            "Installing collected packages: requests-futures, yahooquery\n",
            "Successfully installed requests-futures-1.0.0 yahooquery-2.2.15\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance\n",
        "!pip install yahooquery\n",
        "from yahooquery import Screener\n",
        "import yfinance as yf   \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import random \n",
        "from tensorflow.keras.models import load_model\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IB_YMoe09qVP"
      },
      "outputs": [],
      "source": [
        "clmns = [\n",
        "    \"Open 12-11\",\"Open 11-10\",\"Open 10-9\",\"Open 9-8\",\"Open 8-7\",\"Open 7-6\",\"Open 6-5\",\"Open 5-4\",\"Open 4-3\",\"Open 3-2\",\"Open 2-1\",\n",
        "    \"Close 12-11\",\"Close 11-10\",\"Close 10-9\",\"Close 9-8\",\"Close 8-7\",\"Close 7-6\",\"Close 6-5\",\"Close 5-4\",\"Close 4-3\",\"Close 3-2\",\"Close 2-1\",\n",
        "    \"High 12-11\",\"High 11-10\",\"High 10-9\",\"High 9-8\",\"High 8-7\",\"High 7-6\",\"High 6-5\",\"High 5-4\",\"High 4-3\",\"High 3-2\",\"High 2-1\",\n",
        "    \"Low 12-11\",\"Low 11-10\",\"Low 10-9\",\"Low 9-8\",\"Low 8-7\",\"Low 7-6\",\"Low 6-5\",\"Low 5-4\",\"Low 4-3\",\"Low 3-2\",\"Low 2-1\",\n",
        "    \"AdjV 12-11\",\"AdjV 11-10\",\"AdjV 10-9\",\"AdjV 9-8\",\"AdjV 8-7\",\"AdjV 7-6\",\"AdjV 6-5\",\"AdjV 5-4\",\"AdjV 4-3\",\"AdjV 3-2\",\"AdjV 2-1\",\n",
        "    \"Volume 12-11\",\"Volume 11-10\",\"Volume 10-9\",\"Volume 9-8\",\"Volume 8-7\",\"Volume 7-6\",\"Volume 6-5\",\"Volume 5-4\",\"Volume 4-3\",\"Volume 3-2\",\"Volume 2-1\",\n",
        "    \"suggestion\"]\n",
        "\n",
        "def work_with_dir():\n",
        "  if os.path.exists(\"/content/data/\"):\n",
        "    shutil.rmtree(\"/content/data/\", ignore_errors=True)\n",
        "    print(\"Data Folder Removed\")\n",
        "    os.mkdir(\"/content/data/\")\n",
        "\n",
        "  if not os.path.exists(\"/content/data/\"):\n",
        "    os.mkdir(\"/content/data/\")\n",
        "  \n",
        "  if not os.path.exists(\"/content/extracted/\"):\n",
        "    os.mkdir(\"/content/extracted/\")\n",
        "def read_txt_list():\n",
        "  with open(\"yahoo_stocklist.txt\",\"r\")as f:\n",
        "    lines = f.readlines()\n",
        "    nlines = []\n",
        "    for line in lines:\n",
        "       nlines.append(line.strip())\n",
        "    return nlines\n",
        "def read_syms_from_txt():\n",
        "  with open(\"syms.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "  lst = []\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    lst.append(line)\n",
        "  symbols = lst\n",
        "  return symbols\n",
        "def get_crypto_syms():\n",
        "   # 'all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in',\n",
        "   screens = [\n",
        "       'all_cryptocurrencies_us', 'all_cryptocurrencies_au', 'all_cryptocurrencies_ca', 'all_cryptocurrencies_eu', 'all_cryptocurrencies_gb', 'all_cryptocurrencies_in', ]\n",
        "   s = Screener()\n",
        "   symbols = []\n",
        "   for i in screens:\n",
        "      data = s.get_screeners(i, count=250)\n",
        "      dicts = data[i]['quotes']\n",
        "      syms = [d['symbol'] for d in dicts]\n",
        "      for sym in syms:\n",
        "        symbols.append(sym)\n",
        "   # print(len(symbols))\n",
        "   # pieces = 15\n",
        "   # new_arrays = np.array_split(symbols, pieces)\n",
        "   return symbols\n",
        "def spliting(data):\n",
        "  X = data.drop([\"yes\",\"no\"], axis=1)\n",
        "  y = data[[\"yes\",\"no\"]]\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.2)\n",
        "  print(xTrain.shape, end=\" \")\n",
        "  print(yTrain.shape)\n",
        "  print(xTest.shape, end=\" \")\n",
        "  print(yTest.shape)\n",
        "  return xTrain, xTest, yTrain, yTest\n",
        "def scaler(row):\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    row = scaler.fit_transform(row)\n",
        "    return row\n",
        "def process(data):\n",
        "    data = data.dropna()\n",
        "    row = []\n",
        "    if len(data.columns) == 7:\n",
        "      data = data.iloc[: , 1:]        \n",
        "    data = np.array(data)\n",
        "    llst = [0, 1, 2, 3, 4, 5]\n",
        "    for i in range(12, data.shape[0]):\n",
        "        grow = []\n",
        "        srow = []\n",
        "\n",
        "        for j in llst:\n",
        "           srow.append([\n",
        "               data[i-1][j] - data[i-2][j],\n",
        "               data[i-2][j] - data[i-3][j],\n",
        "               data[i-3][j] - data[i-4][j],\n",
        "               data[i-4][j] - data[i-5][j],\n",
        "               data[i-5][j] - data[i-6][j],\n",
        "               data[i-6][j] - data[i-7][j],\n",
        "               data[i-7][j] - data[i-8][j],\n",
        "               data[i-8][j] - data[i-9][j],\n",
        "               data[i-9][j] - data[i-10][j],\n",
        "               data[i-10][j] - data[i-11][j],\n",
        "               data[i-11][j] - data[i-12][j]\n",
        "           ])\n",
        "\n",
        "        for lst in srow:\n",
        "            mm = np.array(lst)\n",
        "            mm = np.reshape(mm, (-1, 1))\n",
        "            grow.append(scaler(mm))\n",
        "\n",
        "        sugg = \"no\"\n",
        "        if data[i][3] > data[i][0]:\n",
        "            sugg = \"yes\"\n",
        "\n",
        "        arr = np.array(grow).flatten()\n",
        "        arr = np.append(arr, sugg)\n",
        "        row.append(arr)\n",
        "\n",
        "\n",
        "    grow = []\n",
        "    srow = []\n",
        "    llst = []\n",
        "    data = []\n",
        "    arr = []\n",
        "    mm = []\n",
        "\n",
        "    return np.array(row)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data(symbols,periodd,intervall):\n",
        "  \n",
        "  indexx = 100\n",
        "  work_with_dir()\n",
        "  for symbol in symbols:\n",
        "    if ((symbols.index(symbol)+1) % 100 == 0):\n",
        "      print(f\" -- {indexx}\",end=\"\")\n",
        "      indexx = indexx + 100\n",
        "\n",
        "    try:\n",
        "        data = yf.download(symbol, period=periodd,interval=intervall, progress=False,show_errors=False)\n",
        "        if data.empty:\n",
        "           pass\n",
        "        else:\n",
        "           if data.shape[0] > 12:\n",
        "             data.to_csv(f\"/content/data/{symbol}.csv\")\n",
        "             \n",
        "    except:\n",
        "       print(\"Error!\")\n",
        "  print(\" \")\n",
        "  print(f\"Files In Data : {len(os.listdir('/content/data/'))}\")\n",
        "def each_file_proc(files,now,index):\n",
        "     data = []\n",
        "     unattached_dfs = []\n",
        "     files = list(files)\n",
        "     for file in files:\n",
        "        print(f\"{files.index(file)+1+index}\",end=\" \")\n",
        "        if (files.index(file)+index+1) % 40 == 0:\n",
        "          print(\" \")\n",
        "        address = f\"/content/data/{file}\"\n",
        "        unattached_dfs.append(process(pd.read_csv(address)))\n",
        "     data = np.array(unattached_dfs[0])\n",
        "     for i in unattached_dfs[1:]:\n",
        "           data = np.append(data, np.array(i), axis=0)\n",
        "        \n",
        "     unattached_dfs = []\n",
        "  \n",
        "     data = pd.DataFrame(data, columns=clmns)\n",
        "     sugg = data[\"suggestion\"]\n",
        "     data.drop(\"suggestion\",axis=1,inplace=True)\n",
        "     sugg = pd.get_dummies(sugg)\n",
        "     data = pd.concat([data,sugg],axis=1)\n",
        "     data = data.astype(float)\n",
        "     right_now = datetime.now().strftime(\"%H%M%S%f\")\n",
        "     data.to_csv(f\"/content/extracted/{now}/{right_now}.csv\")  \n",
        "def extract_data(pieces):\n",
        "  pd.options.mode.chained_assignment = None\n",
        "\n",
        "  files = os.listdir(\"/content/data/\")\n",
        "  new_files = np.array_split(files, pieces)\n",
        "  print(\"Processing File:\")\n",
        "  now = datetime.now().strftime(\"%H%M%S\")\n",
        "  os.mkdir(f\"/content/extracted/{now}/\")\n",
        "  \n",
        "  index = 0 \n",
        "  for files in new_files:\n",
        "     \n",
        "     each_file_proc(files,now,index)\n",
        "     index = index + len(files)\n",
        "  print(\" \")\n",
        "  return now\n",
        "def delete_all_csv(now):\n",
        "   path = f'/content/extracted/{now}/*.csv'\n",
        "   files = glob.glob(path)\n",
        "   for file in files:\n",
        "       os.remove(file)\n",
        "def make_df(now):\n",
        "   path = f'/content/extracted/{now}/*.parquet'\n",
        "   files = glob.glob(path)\n",
        "   #data = pd.DataFrame()\n",
        "   data = pd.DataFrame()\n",
        "   for adr in files:\n",
        "     data =pd.concat([data,pd.read_parquet(adr)])\n",
        "   if \"Unnamed: 0\" in data:\n",
        "     data.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
        "   print(data.shape)\n",
        "   xTrain,xTest,yTrain,yTest = spliting(data)\n",
        "   data.to_parquet(f'/content/extracted/{now}/data.parquet')\n",
        "   delete_all_csv(now)\n",
        "   data = []\n",
        "   return xTrain,xTest,yTrain,yTest\n",
        "def to_par(now,howmanyfiles): \n",
        "    files = os.listdir(f\"/content/extracted/{now}/\")\n",
        "    addresses = []\n",
        "    for file in files:\n",
        "      addresses.append(f\"/content/extracted/{now}/{file}\")\n",
        "    new_adr = np.array_split(addresses,howmanyfiles)\n",
        "    for adrs in new_adr:\n",
        "      datas = []\n",
        "      for adr in adrs:\n",
        "        datas.append(pd.read_csv(adr))\n",
        "      rnow = datetime.now().strftime(\"%H%M%S%f\")\n",
        "      datas = pd.concat(datas)\n",
        "      datas.to_parquet(f\"/content/extracted/{now}/part_{rnow}.parquet\")      "
      ],
      "metadata": {
        "id": "AMR8z1BIS-M_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIAuU_ILbU27"
      },
      "outputs": [],
      "source": [
        "symbols = get_crypto_syms()\n",
        "#symbols = read_txt_list()\n",
        "#symbols = read_syms_from_txt()\n",
        "#symbols = [\"btc-usd\",\"eth-usd\",\"trx-usd\"]\n",
        "\n",
        "print(f\"Symbols : {len(symbols)}\")\n",
        "download_data(symbols,\"7d\",\"5m\")\n",
        "folder_name = extract_data(100)\n",
        "to_par(folder_name,5)\n",
        "xTrain,xTest,yTrain,yTest = make_df(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xN93WT9e8ueQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003730d9-84af-4034-9d1c-529ac3a983cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_17 (Dense)            (None, 1000)              67000     \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 2)                 2002      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,076,002\n",
            "Trainable params: 7,076,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "\n",
        "\n",
        "model.add(Dense(1000, activation='relu', input_shape=(xTrain.shape[1],)))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adamax()\n",
        "\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_SBxPzRd89uy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c328097a-9355-4114-f267-0c28cfcdf793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1940/1940 [==============================] - 72s 36ms/step - loss: 0.6817 - accuracy: 0.5492 - val_loss: 0.6732 - val_accuracy: 0.5700\n",
            "Epoch 2/30\n",
            "1940/1940 [==============================] - 75s 39ms/step - loss: 0.6571 - accuracy: 0.5957 - val_loss: 0.6477 - val_accuracy: 0.6091\n",
            "Epoch 3/30\n",
            "1940/1940 [==============================] - 70s 36ms/step - loss: 0.6161 - accuracy: 0.6471 - val_loss: 0.6231 - val_accuracy: 0.6470\n",
            "Epoch 4/30\n",
            "1940/1940 [==============================] - 70s 36ms/step - loss: 0.5530 - accuracy: 0.7079 - val_loss: 0.5980 - val_accuracy: 0.6858\n",
            "Epoch 5/30\n",
            "1940/1940 [==============================] - 75s 39ms/step - loss: 0.4792 - accuracy: 0.7646 - val_loss: 0.5844 - val_accuracy: 0.7114\n",
            "Epoch 6/30\n",
            "1940/1940 [==============================] - 70s 36ms/step - loss: 0.4124 - accuracy: 0.8073 - val_loss: 0.5739 - val_accuracy: 0.7314\n",
            "Epoch 7/30\n",
            "1940/1940 [==============================] - 70s 36ms/step - loss: 0.3559 - accuracy: 0.8403 - val_loss: 0.5929 - val_accuracy: 0.7427\n",
            "Epoch 8/30\n",
            "1940/1940 [==============================] - 70s 36ms/step - loss: 0.3099 - accuracy: 0.8644 - val_loss: 0.6030 - val_accuracy: 0.7521\n",
            "Epoch 9/30\n",
            "1940/1940 [==============================] - 70s 36ms/step - loss: 0.2730 - accuracy: 0.8828 - val_loss: 0.6199 - val_accuracy: 0.7550\n",
            "Epoch 10/30\n",
            "1940/1940 [==============================] - 70s 36ms/step - loss: 0.2430 - accuracy: 0.8975 - val_loss: 0.6214 - val_accuracy: 0.7667\n",
            "Epoch 11/30\n",
            "1940/1940 [==============================] - 75s 38ms/step - loss: 0.2165 - accuracy: 0.9097 - val_loss: 0.6924 - val_accuracy: 0.7696\n",
            "Epoch 12/30\n",
            "1940/1940 [==============================] - 75s 39ms/step - loss: 0.1933 - accuracy: 0.9200 - val_loss: 0.7268 - val_accuracy: 0.7710\n",
            "Epoch 13/30\n",
            "1940/1940 [==============================] - 70s 36ms/step - loss: 0.1751 - accuracy: 0.9283 - val_loss: 0.7829 - val_accuracy: 0.7681\n",
            "Epoch 14/30\n",
            "1940/1940 [==============================] - 75s 39ms/step - loss: 0.1612 - accuracy: 0.9343 - val_loss: 0.7826 - val_accuracy: 0.7742\n",
            "Epoch 15/30\n",
            "1940/1940 [==============================] - 75s 39ms/step - loss: 0.1485 - accuracy: 0.9398 - val_loss: 0.7561 - val_accuracy: 0.7819\n",
            "Epoch 16/30\n",
            "1940/1940 [==============================] - 70s 36ms/step - loss: 0.1372 - accuracy: 0.9448 - val_loss: 0.8233 - val_accuracy: 0.7853\n",
            "Epoch 17/30\n",
            "1940/1940 [==============================] - 75s 38ms/step - loss: 0.1282 - accuracy: 0.9489 - val_loss: 0.8391 - val_accuracy: 0.7872\n",
            "Epoch 18/30\n",
            "1940/1940 [==============================] - 75s 39ms/step - loss: 0.1194 - accuracy: 0.9526 - val_loss: 0.8431 - val_accuracy: 0.7886\n",
            "Epoch 19/30\n",
            "1940/1940 [==============================] - 75s 39ms/step - loss: 0.1128 - accuracy: 0.9555 - val_loss: 0.8369 - val_accuracy: 0.7908\n",
            "Epoch 20/30\n",
            "1940/1940 [==============================] - 75s 39ms/step - loss: 0.1055 - accuracy: 0.9586 - val_loss: 0.8844 - val_accuracy: 0.7904\n",
            "Epoch 21/30\n",
            "1940/1940 [==============================] - 75s 39ms/step - loss: 0.0988 - accuracy: 0.9612 - val_loss: 0.8497 - val_accuracy: 0.7928\n",
            "Epoch 22/30\n",
            "1940/1940 [==============================] - 75s 39ms/step - loss: 0.0925 - accuracy: 0.9641 - val_loss: 0.8534 - val_accuracy: 0.7951\n",
            "Epoch 23/30\n",
            "1940/1940 [==============================] - 71s 36ms/step - loss: 0.0873 - accuracy: 0.9662 - val_loss: 0.8702 - val_accuracy: 0.7947\n",
            "Epoch 24/30\n",
            "1940/1940 [==============================] - 75s 39ms/step - loss: 0.0831 - accuracy: 0.9679 - val_loss: 0.9295 - val_accuracy: 0.7928\n",
            "Epoch 25/30\n",
            "1940/1940 [==============================] - 75s 39ms/step - loss: 0.0795 - accuracy: 0.9695 - val_loss: 0.9460 - val_accuracy: 0.7954\n",
            "Epoch 26/30\n",
            "1940/1940 [==============================] - 70s 36ms/step - loss: 0.0757 - accuracy: 0.9710 - val_loss: 0.9360 - val_accuracy: 0.7958\n",
            "Epoch 27/30\n",
            "1940/1940 [==============================] - 75s 39ms/step - loss: 0.0722 - accuracy: 0.9724 - val_loss: 0.9531 - val_accuracy: 0.7981\n",
            "Epoch 28/30\n",
            "1940/1940 [==============================] - 71s 36ms/step - loss: 0.0696 - accuracy: 0.9735 - val_loss: 0.9394 - val_accuracy: 0.7998\n",
            "Epoch 29/30\n",
            "1940/1940 [==============================] - 70s 36ms/step - loss: 0.0669 - accuracy: 0.9746 - val_loss: 0.9534 - val_accuracy: 0.8019\n",
            "Epoch 30/30\n",
            "1940/1940 [==============================] - 75s 38ms/step - loss: 0.0646 - accuracy: 0.9756 - val_loss: 0.9842 - val_accuracy: 0.8019\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f74c784be50>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model.fit(xTrain,yTrain,epochs=30,batch_size=1000,validation_data=(xTest,yTest),shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict([[ 0.0852705 , -0.07649586, -0.21869615,  0.01526907, -1.        ,\n",
        "        0.01591626, -0.08460826, -0.02246337,  1.        , -0.18438024,\n",
        "       -0.12173867,  0.16809759,  0.12604778, -0.10042974,  0.07407236,\n",
        "       -1.        ,  0.06135576,  0.13767386,  0.03536805,  0.11259184,\n",
        "        0.16970565,  1.        ,  0.15213167,  0.07138407,  0.00860105,\n",
        "        0.00506765,  0.07134688, -1.        ,  0.12916454,  1.        ,\n",
        "        0.40506765, -0.04345158,  0.04216839,  0.03269911,  0.11635447,\n",
        "       -0.02678805, -0.09251375,  0.10104467, -1.        ,  0.16988145,\n",
        "       -0.07676185,  0.06965549,  0.12025151,  1.        ,  0.03269911,\n",
        "        0.11635447, -0.02678805, -0.09251375,  0.10104467, -1.        ,\n",
        "        0.16988145, -0.07676185,  0.06965549,  0.12025151,  1.        ,\n",
        "        0.        ,  0.        ,  0.        , -0.48554291,  0.48554291,\n",
        "       -1.        ,  1.        ,  0.        , -0.35358351, -0.08658874,\n",
        "        0.44017225]])"
      ],
      "metadata": {
        "id": "awUWSkq2wWca",
        "outputId": "5ef5f927-6d18-4d46-9d7c-3d92fc9f39da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02168633, 0.966065  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"cmmv1.h5\")"
      ],
      "metadata": {
        "id": "ibVpZDnk0DB3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8CGI7G0bxqG",
        "outputId": "f78b9c4d-8706-43c2-c009-1ebe628e862b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CMm_v1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}